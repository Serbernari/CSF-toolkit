{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLIES:\n",
      "  [+0] Can you be more specific about which constraints you think it ignores? Might help us adjust the proposal.\n",
      "  [+1] Agree—right now it reads like we’re assuming infinite time/resources. We should list the constraints explicitly.\n",
      "  [+0] I’m not convinced it’s a bad idea, but we definitely need a clearer constraints section. What are the top 2 blockers you see?\n",
      "  [+0] This feels a bit hand-wavy. Which constraints (latency, security, staffing, budget) are you referring to?\n",
      "  [+0] If we can’t meet the constraints, then the proposal needs a Plan B or phased rollout. Otherwise it’s not actionable.\n",
      "  [-1] I get the concern, but calling it a “bad idea” without details isn’t super helpful. Can you point to concrete examples?\n",
      "  [+0] From my side, the proposal does address some constraints, just not thoroughly. Maybe we should add a risk/assumptions table.\n",
      "  [+1] Strongly agree. We’ve tried similar approaches before and hit the same hard limits—let’s not repeat that without mitigation.\n",
      "\n",
      "CSFResult:\n",
      "CSFResult(reception={'ponos': 0.125, 'ponos_ci_low': 0.02241690886329617, 'ponos_ci_high': 0.4708948057698615, 'ponos_net': 0.125, 'ponos_early_5': 0.0, 'ponos_weighted_by_length': 0.12795698924731183, 'n_replies': 8}, intrinsic={'toxicity': 0.0})\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Mapping, Sequence\n",
    "from dataclasses import asdict\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, conlist, confloat\n",
    "from typing_extensions import Literal\n",
    "\n",
    "# ✅ your library imports\n",
    "from csf_toolkit import Thread, Reply, CSFResult, ponos, ponos_net, ponos_early, ponos_weighted\n",
    "from csf_toolkit.reaction import ReactionGenerator\n",
    "from csf_toolkit.sentiment import SentimentScorer\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# OpenAI-backed implementations\n",
    "# ----------------------------\n",
    "class RepliesOut(BaseModel):\n",
    "    replies: list[str] = Field(..., description=\"Exactly n plausible replies as strings.\")\n",
    "\n",
    "\n",
    "class LabelsOut(BaseModel):\n",
    "    labels: list[Literal[-1, 0, 1]] = Field(..., description=\"One label per reply in order.\")\n",
    "\n",
    "\n",
    "class ToxicityOut(BaseModel):\n",
    "    toxicity: confloat(ge=0.0, le=1.0)  # type: ignore\n",
    "\n",
    "\n",
    "class OpenAIReactionGenerator:\n",
    "    \"\"\"Implements csf_toolkit.reaction.ReactionGenerator via OpenAI.\"\"\"\n",
    "    def __init__(self, client: OpenAI, *, model: str = \"gpt-5.2\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def generate_replies(\n",
    "        self,\n",
    "        *,\n",
    "        comment: str,\n",
    "        topic: str | None = None,\n",
    "        n: int = 8,\n",
    "        context: Mapping[str, Any] | None = None,\n",
    "    ) -> list[str]:\n",
    "        context = dict(context or {})\n",
    "\n",
    "        system = (\n",
    "            \"You generate plausible short replies to an online comment.\\n\"\n",
    "            \"Return a realistic mix of supportive, neutral, and critical responses.\\n\"\n",
    "            \"Safety: do NOT include slurs, hate speech, or explicit threats.\\n\"\n",
    "            \"Keep negativity mild (disagreement, annoyance, criticism).\\n\"\n",
    "            f\"Return exactly {n} replies.\"\n",
    "        )\n",
    "\n",
    "        user = (\n",
    "            f\"TOPIC: {topic or ''}\\n\"\n",
    "            f\"COMMENT: {comment}\\n\"\n",
    "            f\"CONTEXT_JSON: {context}\\n\"\n",
    "            f\"n={n}\\n\"\n",
    "            \"Generate replies.\"\n",
    "        )\n",
    "\n",
    "        resp = self.client.responses.parse(\n",
    "            model=self.model,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            text_format=RepliesOut,\n",
    "        )\n",
    "\n",
    "        out = resp.output_parsed\n",
    "        if out is None:\n",
    "            raise RuntimeError(\"Model returned no parsed output (possibly a refusal).\")\n",
    "        return [r.strip() for r in out.replies]\n",
    "\n",
    "\n",
    "class OpenAISentimentScorer:\n",
    "    \"\"\"Implements csf_toolkit.sentiment.SentimentScorer via OpenAI (single call for all replies).\"\"\"\n",
    "    def __init__(self, client: OpenAI, *, model: str = \"gpt-5.2\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def score_replies(\n",
    "        self,\n",
    "        *,\n",
    "        topic: str | None,\n",
    "        comment: str,\n",
    "        replies: Sequence[str],\n",
    "        context: dict | None = None,\n",
    "    ) -> list[int]:\n",
    "        # Build a schema with the right length (Pydantic will still parse, prompt enforces exactness)\n",
    "        n = len(replies)\n",
    "\n",
    "        # We'll validate length ourselves after parse:\n",
    "        class _LabelsN(BaseModel):\n",
    "            labels: conlist(Literal[-1, 0, 1], min_length=n, max_length=n)  # type: ignore\n",
    "\n",
    "        context = dict(context or {})\n",
    "\n",
    "        system = (\n",
    "            \"You are a sentiment labeler for replies in a conversation.\\n\"\n",
    "            \"Label each reply's sentiment toward the COMMENT as:\\n\"\n",
    "            \"  -1 = Negative (hostile, insulting, dismissive, angry criticism)\\n\"\n",
    "            \"   0 = Neutral (informational, unclear, mixed, questions)\\n\"\n",
    "            \"  +1 = Positive (supportive, agreeing, empathetic)\\n\"\n",
    "            \"Return ONLY the labels list; one label per reply, in order.\"\n",
    "        )\n",
    "\n",
    "        numbered = \"\\n\".join([f\"{i+1}. {t}\" for i, t in enumerate(replies)])\n",
    "\n",
    "        user = (\n",
    "            f\"TOPIC: {topic or ''}\\n\"\n",
    "            f\"COMMENT: {comment}\\n\"\n",
    "            f\"CONTEXT_JSON: {context}\\n\"\n",
    "            \"REPLIES:\\n\"\n",
    "            f\"{numbered}\\n\"\n",
    "            \"Output labels.\"\n",
    "        )\n",
    "\n",
    "        resp = self.client.responses.parse(\n",
    "            model=self.model,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user},\n",
    "            ],\n",
    "            text_format=_LabelsN,\n",
    "        )\n",
    "\n",
    "        out = resp.output_parsed\n",
    "        if out is None:\n",
    "            raise RuntimeError(\"Model returned no parsed output (possibly a refusal).\")\n",
    "\n",
    "        labels = [int(x) for x in out.labels]\n",
    "        if len(labels) != n:\n",
    "            raise ValueError(f\"Expected {n} labels, got {len(labels)}\")\n",
    "        return labels\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpful wrapper to avoid repeated API calls\n",
    "# (your metrics call scorer multiple times)\n",
    "# ----------------------------\n",
    "class PrefixCachingScorer:\n",
    "    \"\"\"\n",
    "    Wraps a SentimentScorer and caches results.\n",
    "    Also reuses cached labels for prefix reply-lists (so ponos_early doesn't cost another call).\n",
    "    \"\"\"\n",
    "    def __init__(self, inner: SentimentScorer):\n",
    "        self.inner = inner\n",
    "        # key_base -> list of (replies_tuple, labels)\n",
    "        self._cache: dict[tuple, list[tuple[tuple[str, ...], list[int]]]] = {}\n",
    "\n",
    "    def score_replies(\n",
    "        self,\n",
    "        *,\n",
    "        topic: str | None,\n",
    "        comment: str,\n",
    "        replies: Sequence[str],\n",
    "        context: dict | None = None,\n",
    "    ) -> list[int]:\n",
    "        context = dict(context or {})\n",
    "        key_base = (topic, comment, tuple(sorted(context.items())))\n",
    "        r = tuple(replies)\n",
    "\n",
    "        bucket = self._cache.setdefault(key_base, [])\n",
    "\n",
    "        # exact match\n",
    "        for r_full, labs_full in bucket:\n",
    "            if r_full == r:\n",
    "                return labs_full\n",
    "\n",
    "        # prefix match (reuse)\n",
    "        for r_full, labs_full in bucket:\n",
    "            if len(r) <= len(r_full) and r_full[: len(r)] == r:\n",
    "                return labs_full[: len(r)]\n",
    "\n",
    "        labs = self.inner.score_replies(topic=topic, comment=comment, replies=replies, context=context)\n",
    "        bucket.append((r, labs))\n",
    "        return labs\n",
    "\n",
    "\n",
    "def intrinsic_toxicity_gpt(\n",
    "    client: OpenAI,\n",
    "    *,\n",
    "    comment: str,\n",
    "    topic: str | None = None,\n",
    "    context: Mapping[str, Any] | None = None,\n",
    "    model: str = \"gpt-5.2\",\n",
    ") -> float:\n",
    "    \"\"\"Optional intrinsic score in [0,1] from GPT (heuristic, not reception-based).\"\"\"\n",
    "    context = dict(context or {})\n",
    "    system = (\n",
    "        \"Score the COMMENT for text-intrinsic toxicity on a continuous scale [0,1].\\n\"\n",
    "        \"0 = benign; 1 = severe abuse/threats/hate.\\n\"\n",
    "        \"Return only a JSON object with the key 'toxicity'.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"TOPIC: {topic or ''}\\n\"\n",
    "        f\"COMMENT: {comment}\\n\"\n",
    "        f\"CONTEXT_JSON: {context}\\n\"\n",
    "        \"Return toxicity.\"\n",
    "    )\n",
    "\n",
    "    resp = client.responses.parse(\n",
    "        model=model,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        text_format=ToxicityOut,\n",
    "    )\n",
    "    out = resp.output_parsed\n",
    "    if out is None:\n",
    "        raise RuntimeError(\"Model returned no parsed output (possibly a refusal).\")\n",
    "    return float(out.toxicity)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Full CSF pipeline using your library\n",
    "# ----------------------------\n",
    "def run_csf_openai_pipeline(\n",
    "    *,\n",
    "    comment: str,\n",
    "    topic: str | None = None,\n",
    "    context: Mapping[str, Any] | None = None,\n",
    "    n_replies: int = 8,\n",
    "    model: str = \"gpt-5.2\",\n",
    ") -> tuple[CSFResult, Thread, list[int]]:\n",
    "    client = OpenAI(api_key=\"\")\n",
    "\n",
    "    generator: ReactionGenerator = OpenAIReactionGenerator(client, model=model)\n",
    "    scorer: SentimentScorer = PrefixCachingScorer(OpenAISentimentScorer(client, model=model))\n",
    "\n",
    "    # 1) Generate plausible replies (OpenAI)\n",
    "    replies_text = generator.generate_replies(comment=comment, topic=topic, n=n_replies, context=context)\n",
    "\n",
    "    # 2) Build a real Thread/Reply from your library (no hacks)\n",
    "    thread = Thread(\n",
    "        comment=comment,\n",
    "        topic=topic,\n",
    "        context=dict(context or {}),\n",
    "        replies=[Reply(text=t, metadata={\"length\": len(t)}) for t in replies_text],\n",
    "    )\n",
    "\n",
    "    # 3) Reception metrics (your library)\n",
    "    p, ci = ponos(thread, scorer, return_ci=True)\n",
    "    reception = {\n",
    "        \"ponos\": float(p),\n",
    "        \"ponos_ci_low\": float(ci.lower),\n",
    "        \"ponos_ci_high\": float(ci.upper),\n",
    "        \"ponos_net\": float(ponos_net(thread, scorer)),\n",
    "        \"ponos_early_5\": float(ponos_early(thread, scorer, k=5)),\n",
    "        \"ponos_weighted_by_length\": float(\n",
    "            ponos_weighted(thread, scorer, weight_fn=lambda md: float(md.get(\"length\", 1.0)))\n",
    "        ),\n",
    "        \"n_replies\": len(thread.replies),\n",
    "    }\n",
    "\n",
    "    # 4) Intrinsic score (OpenAI)\n",
    "    intrinsic = {\"toxicity\": intrinsic_toxicity_gpt(client, comment=comment, topic=topic, context=context, model=model)}\n",
    "\n",
    "    # 5) CSFResult (your library)\n",
    "    result = CSFResult(reception=reception, intrinsic=intrinsic)\n",
    "\n",
    "    # labels (optional: useful for inspection)\n",
    "    labels = scorer.score_replies(topic=topic, comment=comment, replies=[r.text for r in thread.replies], context=dict(thread.context))\n",
    "    return result, thread, labels\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example run (Jupyter-friendly)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    comment = \"I think this proposal is a bad idea because it ignores basic constraints.\"\n",
    "    result, thread, labels = run_csf_openai_pipeline(\n",
    "        comment=comment,\n",
    "        topic=\"workplace discussion\",\n",
    "        context={\"platform\": \"internal_forum\", \"audience\": \"engineers\"},\n",
    "        n_replies=8,\n",
    "        model=\"gpt-5.2\",\n",
    "    )\n",
    "\n",
    "    print(\"REPLIES:\")\n",
    "    for r, lab in zip(thread.replies, labels):\n",
    "        print(f\"  [{lab:+}] {r.text}\")\n",
    "\n",
    "    print(\"\\nCSFResult:\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407211c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee112fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cc461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSF_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
